## 爬取知乎[“轮子哥”——vczh](https://www.zhihu.com/people/excited-vczh/following)所有的关注对象

**在Network中查看响应文件，找到含有我们需要的信息的文件。最终找到一个followee的文件，里面包含了该页所有关注者的信息(图1)，而这些信息是通过直接请求知乎API 得到的(图2)，我们就可以通过对这个URL请求从而过滤掉其他们无用的数据，只获取我们需要的信息**

图1：

![image](https://github.com/suvieu/PYTHON-PROGRAM/blob/master/SCRAPING/ZHIHU/PIC/1.png)


图2：

![image](https://github.com/suvieu/PYTHON-PROGRAM/blob/master/SCRAPING/ZHIHU/PIC/2.png)

**翻页，寻找每一页URL的规律**

第一页关注者的URL:
https://www.zhihu.com/api/v4/members/excited-vczh/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&offset=20&limit=20

第二页关注着的URL
https://www.zhihu.com/api/v4/members/excited-vczh/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&offset=40&limit=20

**比较发现最后offset=40&limit=20是决定页面变化的参数，以20一页为限制，offset为0时显示第一页，20显示第二页，以此类推，所以要爬取每一页的信息只要把这个参数改成变量，写一个for循环即可；**

```python
for i in range(2): #这里只爬去前两页
    print("当前正在爬去第{}页".format(i+1))
    url_2 = 'https://www.zhihu.com/api/v4/members/excited-vczh/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&offset={}&limit=20'.format(i*20)
    response = requests.get(url_2, headers=headers)
 ```
**这样我们基本就已经获取到我们想要的信息了，接下来是对获取数据的格式做处理，先用jason.load把爬取下来的数据转变成字典**

```python
data = json.loads(response.text)
```
**注意这里的字典是嵌套式字典，字典中还有字典或者列表(如图3),我们其实只需要键“data”下面的value就够了**
```python
user_data.extend(data['data'])
```

图3：

![image](https://github.com/suvieu/PYTHON-PROGRAM/blob/master/SCRAPING/ZHIHU/PIC/4.jpg)


**最后再把所有数据通过pandas导出成excel就完成了**

```python
df = pd.DataFrame.from_dict(user_data)
df.to_excel('user_data.xlsx')
```

[点击查看完整代码](https://github.com/suvieu/PYTHON-PROGRAM/blob/master/SCRAPING/ZHIHU/zhihu_followee.py)
